"""
Vector Database Service - LangChain + FAISS
PDF ë¬¸ì„œë¥¼ ì²­í‚¹í•˜ê³  ì„ë² ë”©í•˜ì—¬ FAISS ë²¡í„° DBì— ì €ì¥
"""

import os
import pickle
from typing import List, Dict, Any, Optional
from pathlib import Path

from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.schema import Document

from config.settings import OPENAI_API_KEY, UPLOAD_DIR


class VectorDBService:
    """
    LangChain + FAISSë¥¼ í™œìš©í•œ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì„œë¹„ìŠ¤

    ì£¼ìš” ê¸°ëŠ¥:
    - PDF í…ìŠ¤íŠ¸ ì²­í‚¹ (RecursiveCharacterTextSplitter)
    - OpenAI Embeddings APIë¥¼ í†µí•œ ì„ë² ë”© ìƒì„± (text-embedding-3-small)
    - FAISS ë²¡í„° ìŠ¤í† ì–´ ìƒì„± ë° ê´€ë¦¬
    - ì‹œë§¨í‹± ê²€ìƒ‰
    """

    def __init__(self, api_key: Optional[str] = None):
        """
        VectorDBService ì´ˆê¸°í™”

        Args:
            api_key: OpenAI API í‚¤ (Noneì´ë©´ ì„¤ì •ì—ì„œ ê°€ì ¸ì˜´)
        """
        self.api_key = api_key or OPENAI_API_KEY

        if not self.api_key:
            raise ValueError("OPENAI_API_KEY is required for embeddings")

        # OpenAI Embeddings ì´ˆê¸°í™” (text-embedding-3-small ëª¨ë¸)
        self.embeddings = OpenAIEmbeddings(
            model="text-embedding-3-small",
            openai_api_key=self.api_key
        )

        # í…ìŠ¤íŠ¸ ìŠ¤í”Œë¦¬í„° ì´ˆê¸°í™”
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,  # ì²­í¬ í¬ê¸° (í† í° ìˆ˜)
            chunk_overlap=200,  # ì²­í¬ ê°„ ì˜¤ë²„ë©
            length_function=len,
            separators=["\n\n", "\n", ". ", " ", ""]
        )

        # ë²¡í„° ìŠ¤í† ì–´ ë””ë ‰í† ë¦¬
        self.vector_store_dir = os.path.join(UPLOAD_DIR, "vector_stores")
        os.makedirs(self.vector_store_dir, exist_ok=True)

        # í˜„ì¬ ë¡œë“œëœ ë²¡í„° ìŠ¤í† ì–´ë“¤ (íŒŒì¼ëª… -> FAISS ê°ì²´)
        self.vector_stores: Dict[str, FAISS] = {}

    def create_vector_store_from_text(
        self,
        text: str,
        file_name: str,
        metadata: Optional[Dict[str, Any]] = None
    ) -> FAISS:
        """
        í…ìŠ¤íŠ¸ë¡œë¶€í„° ë²¡í„° ìŠ¤í† ì–´ ìƒì„±

        Args:
            text: ì „ì²´ í…ìŠ¤íŠ¸
            file_name: íŒŒì¼ ì´ë¦„ (ë²¡í„° ìŠ¤í† ì–´ ì‹ë³„ìš©)
            metadata: ë©”íƒ€ë°ì´í„° (ì„ íƒì‚¬í•­)

        Returns:
            ìƒì„±ëœ FAISS ë²¡í„° ìŠ¤í† ì–´
        """
        print(f"ğŸ“š ë²¡í„° ìŠ¤í† ì–´ ìƒì„± ì‹œì‘: {file_name}")

        # 1. í…ìŠ¤íŠ¸ ì²­í‚¹
        print(f"   âœ‚ï¸ í…ìŠ¤íŠ¸ ì²­í‚¹ ì¤‘...")
        chunks = self.text_splitter.split_text(text)
        print(f"   âœ… {len(chunks)}ê°œ ì²­í¬ ìƒì„± ì™„ë£Œ")

        # 2. Document ê°ì²´ ìƒì„± (ë©”íƒ€ë°ì´í„° í¬í•¨)
        documents = []
        for idx, chunk in enumerate(chunks):
            doc_metadata = {
                "source": file_name,
                "chunk_index": idx,
                "total_chunks": len(chunks)
            }
            if metadata:
                doc_metadata.update(metadata)

            documents.append(Document(
                page_content=chunk,
                metadata=doc_metadata
            ))

        # 3. FAISS ë²¡í„° ìŠ¤í† ì–´ ìƒì„±
        print(f"   ğŸ”¢ ì„ë² ë”© ìƒì„± ë° FAISS ì¸ë±ìŠ¤ êµ¬ì¶• ì¤‘...")
        vector_store = FAISS.from_documents(
            documents=documents,
            embedding=self.embeddings
        )
        print(f"   âœ… ë²¡í„° ìŠ¤í† ì–´ ìƒì„± ì™„ë£Œ")

        # 4. ë©”ëª¨ë¦¬ì— ì €ì¥
        self.vector_stores[file_name] = vector_store

        # 5. ë””ìŠ¤í¬ì— ì €ì¥
        self.save_vector_store(file_name, vector_store)

        return vector_store

    def save_vector_store(self, file_name: str, vector_store: FAISS) -> str:
        """
        ë²¡í„° ìŠ¤í† ì–´ë¥¼ ë””ìŠ¤í¬ì— ì €ì¥

        Args:
            file_name: íŒŒì¼ ì´ë¦„
            vector_store: FAISS ë²¡í„° ìŠ¤í† ì–´

        Returns:
            ì €ì¥ëœ ê²½ë¡œ
        """
        # íŒŒì¼ëª…ì—ì„œ í™•ì¥ì ì œê±° ë° ì•ˆì „í•œ ê²½ë¡œ ìƒì„±
        safe_name = Path(file_name).stem
        store_path = os.path.join(self.vector_store_dir, safe_name)

        # FAISS ì €ì¥ (ìë™ìœ¼ë¡œ .faissì™€ .pkl íŒŒì¼ ìƒì„±)
        vector_store.save_local(store_path)

        print(f"ğŸ’¾ ë²¡í„° ìŠ¤í† ì–´ ì €ì¥ ì™„ë£Œ: {store_path}")
        return store_path

    def load_vector_store(self, file_name: str) -> Optional[FAISS]:
        """
        ë””ìŠ¤í¬ì—ì„œ ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œ

        Args:
            file_name: íŒŒì¼ ì´ë¦„

        Returns:
            ë¡œë“œëœ FAISS ë²¡í„° ìŠ¤í† ì–´ (ì—†ìœ¼ë©´ None)
        """
        # ì´ë¯¸ ë©”ëª¨ë¦¬ì— ìˆìœ¼ë©´ ë°˜í™˜
        if file_name in self.vector_stores:
            print(f"ğŸ“¦ ë²¡í„° ìŠ¤í† ì–´ ìºì‹œì—ì„œ ë¡œë“œ: {file_name}")
            return self.vector_stores[file_name]

        # ë””ìŠ¤í¬ì—ì„œ ë¡œë“œ
        safe_name = Path(file_name).stem
        store_path = os.path.join(self.vector_store_dir, safe_name)

        if not os.path.exists(store_path):
            print(f"âš ï¸ ë²¡í„° ìŠ¤í† ì–´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {store_path}")
            return None

        try:
            print(f"ğŸ“‚ ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œ ì¤‘: {store_path}")
            vector_store = FAISS.load_local(
                store_path,
                embeddings=self.embeddings,
                allow_dangerous_deserialization=True  # pickle ì—­ì§ë ¬í™” í—ˆìš©
            )

            # ë©”ëª¨ë¦¬ì— ìºì‹œ
            self.vector_stores[file_name] = vector_store
            print(f"âœ… ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œ ì™„ë£Œ")

            return vector_store

        except Exception as e:
            print(f"âŒ ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
            return None

    def search(
        self,
        query: str,
        file_name: str,
        top_k: int = 5
    ) -> List[Dict[str, Any]]:
        """
        ë²¡í„° ìŠ¤í† ì–´ì—ì„œ ì‹œë§¨í‹± ê²€ìƒ‰

        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            file_name: ê²€ìƒ‰í•  íŒŒì¼ ì´ë¦„
            top_k: ë°˜í™˜í•  ìµœëŒ€ ê²°ê³¼ ìˆ˜

        Returns:
            ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ (ê° ê²°ê³¼ëŠ” {'text', 'metadata', 'score'} í¬í•¨)
        """
        # ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œ
        vector_store = self.load_vector_store(file_name)

        if vector_store is None:
            print(f"âš ï¸ ë²¡í„° ìŠ¤í† ì–´ê°€ ì—†ì–´ ê²€ìƒ‰ ë¶ˆê°€: {file_name}")
            return []

        print(f"ğŸ” ì‹œë§¨í‹± ê²€ìƒ‰ ì‹œì‘: '{query}' (top_k={top_k})")

        # ìœ ì‚¬ë„ ê²€ìƒ‰ (ì ìˆ˜ í¬í•¨)
        results = vector_store.similarity_search_with_score(
            query=query,
            k=top_k
        )

        # ê²°ê³¼ í¬ë§·íŒ…
        formatted_results = []
        for doc, score in results:
            formatted_results.append({
                "text": doc.page_content,
                "metadata": doc.metadata,
                "score": float(score)  # FAISSëŠ” ê±°ë¦¬(ë‚®ì„ìˆ˜ë¡ ìœ ì‚¬)ë¥¼ ë°˜í™˜
            })

        print(f"âœ… {len(formatted_results)}ê°œ ê²°ê³¼ ì°¾ìŒ")

        return formatted_results

    def delete_vector_store(self, file_name: str) -> bool:
        """
        ë²¡í„° ìŠ¤í† ì–´ ì‚­ì œ (ë©”ëª¨ë¦¬ ë° ë””ìŠ¤í¬)

        Args:
            file_name: íŒŒì¼ ì´ë¦„

        Returns:
            ì‚­ì œ ì„±ê³µ ì—¬ë¶€
        """
        try:
            # ë©”ëª¨ë¦¬ì—ì„œ ì‚­ì œ
            if file_name in self.vector_stores:
                del self.vector_stores[file_name]

            # ë””ìŠ¤í¬ì—ì„œ ì‚­ì œ
            safe_name = Path(file_name).stem
            store_path = os.path.join(self.vector_store_dir, safe_name)

            if os.path.exists(store_path):
                import shutil
                shutil.rmtree(store_path)
                print(f"ğŸ—‘ï¸ ë²¡í„° ìŠ¤í† ì–´ ì‚­ì œ ì™„ë£Œ: {file_name}")
                return True

            return False

        except Exception as e:
            print(f"âŒ ë²¡í„° ìŠ¤í† ì–´ ì‚­ì œ ì‹¤íŒ¨: {str(e)}")
            return False

    def list_vector_stores(self) -> List[str]:
        """
        ì €ì¥ëœ ë²¡í„° ìŠ¤í† ì–´ ëª©ë¡ ì¡°íšŒ

        Returns:
            ë²¡í„° ìŠ¤í† ì–´ ì´ë¦„ ë¦¬ìŠ¤íŠ¸
        """
        try:
            if not os.path.exists(self.vector_store_dir):
                return []

            stores = [
                d for d in os.listdir(self.vector_store_dir)
                if os.path.isdir(os.path.join(self.vector_store_dir, d))
            ]

            return stores

        except Exception as e:
            print(f"âŒ ë²¡í„° ìŠ¤í† ì–´ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
            return []

    def get_store_info(self, file_name: str) -> Optional[Dict[str, Any]]:
        """
        ë²¡í„° ìŠ¤í† ì–´ ì •ë³´ ì¡°íšŒ

        Args:
            file_name: íŒŒì¼ ì´ë¦„

        Returns:
            ë²¡í„° ìŠ¤í† ì–´ ì •ë³´ (ì´ ì²­í¬ ìˆ˜ ë“±)
        """
        vector_store = self.load_vector_store(file_name)

        if vector_store is None:
            return None

        # FAISS ì¸ë±ìŠ¤ì—ì„œ ë²¡í„° ê°œìˆ˜ í™•ì¸
        total_vectors = vector_store.index.ntotal

        return {
            "file_name": file_name,
            "total_chunks": total_vectors,
            "embedding_dimension": vector_store.index.d
        }


# Global VectorDBService instance (ì‹±ê¸€í†¤ íŒ¨í„´)
_vector_db_service_instance: Optional[VectorDBService] = None


def get_vector_db_service() -> VectorDBService:
    """
    VectorDBService ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜

    Returns:
        VectorDBService ì¸ìŠ¤í„´ìŠ¤
    """
    global _vector_db_service_instance

    if _vector_db_service_instance is None:
        _vector_db_service_instance = VectorDBService()

    return _vector_db_service_instance
